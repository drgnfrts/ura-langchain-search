{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown Cleaning with GPT4-o\n",
    "\n",
    "(Auto Scrape Iteration 2 Part 2)\n",
    "\n",
    "With the Assistants feature on OpenAI, we call the API and give the LLM a set of instructions to parse and clean the Markdown files converted by Pandoc in the previous notebook. While GPT is able to fix some issues such as inconsistent formatting, the cleaning is not very successful especially for very large documents. Nevertheless this was a good opportunity to get to know OpenAI's Assistants, in particular its File Search capability.\n",
    "\n",
    "For this notebook, you will need to replace os.environ.get(\"KEYNAME) with your OpenAI key directly in the notebook since Jupyter Notebook is unable to read from the environment variables file even within the same root/project folder.\n",
    "\n",
    "This notebook is a documented copy of the original _batch\\_md\\_clean.py_ and _add\\_metadata.py_ files, which are now deprecated and removed from the repo.\n",
    "\n",
    "Import libraries as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the OpenAI client and retrieve the assistant and vector store. Uncomment the setup if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"URA_OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"\"\n",
    "# with open('../data/working_assistant_prompt.txt', 'r') as f:\n",
    "#     for line in f.readlines():\n",
    "#         prompt += line.strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assistant = client.beta.assistants.create(\n",
    "#   name=\"URA Assistant\",\n",
    "#   instructions=prompt,\n",
    "#   model=\"gpt-4o\",\n",
    "#   tools=[{\"type\": \"file_search\"}],\n",
    "# )\n",
    "\n",
    "# vector_store = client.beta.vector_stores.create(name=\"URA_Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve and update the assistants model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.retrieve(\n",
    "    assistant_id=os.environ.get(\"URA_MARKDOWN_ASSISTANT_ID\"))\n",
    "vector_store = client.beta.vector_stores.retrieve(\n",
    "    vector_store_id=os.environ.get(\"URA_MARKDOWN_CLEANER_VECTOR_STORE_ID\"))\n",
    "\n",
    "# Update the assistant with the vector store information\n",
    "client.beta.assistants.update(\n",
    "    model=\"gpt-4o\",\n",
    "    assistant_id=assistant.id,\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    "    temperature=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to find the metadata (title, link, last updated date) are as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(content):\n",
    "    \"\"\"\n",
    "    Extracts the text starting from 'Last updated on' to the specified year '2024', inclusively.\n",
    "\n",
    "    Parameters:\n",
    "        content (str): Main content of the webpage\n",
    "\n",
    "    Returns\n",
    "        date_string (str): Last updated date as extracted\n",
    "        content (str): Main content of the webpage with last updated date removed to avoid duplication\n",
    "    \"\"\"\n",
    "    pattern = r\".*Last updated on(.*\\d{4}).*\"\n",
    "    match = re.search(pattern, content)\n",
    "    try:\n",
    "        date_string = match.group(1).replace('*', '')\n",
    "    except:\n",
    "        date_string = 'No date found'\n",
    "    content = re.sub(pattern, '', content)\n",
    "    return date_string, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_link(file_path):\n",
    "    \"\"\"\n",
    "    Formats the link based on the file path.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Full path to the specified file\n",
    "\n",
    "    Returns:\n",
    "        (str): Full URL of webpage whose contents are being processed. This works because the directory structure replicates the website structure.\n",
    "    \"\"\"\n",
    "    base_url = 'https://www.ura.gov.sg/Corporate/Guidelines/Development-Control'\n",
    "    relative_path = file_path.split(\n",
    "        'DC-cleaned-md')[-1].replace('\\\\', '/').lstrip('/').replace('.md', '')\n",
    "    return f'{base_url}/{relative_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_yaml_metadata(content, file_path):\n",
    "    \"\"\"\n",
    "    Reads file, extracts required info, and writes back with YAML metadata.\n",
    "\n",
    "    Parameters:\n",
    "        content (str): Main content of the webpage\n",
    "        file_path (str): Full path to the specified file\n",
    "\n",
    "    Returns\n",
    "        (str): Metadata in YAML format\n",
    "    \n",
    "    \"\"\"\n",
    "    # Assumes the first line is the title\n",
    "\n",
    "    title = content.splitlines()[0].lstrip('#').strip()\n",
    "    link = format_link(file_path)\n",
    "    date, content = extract_date(content)\n",
    "    date = date.strip()\n",
    "\n",
    "    # Create YAML metadata\n",
    "    return f\"---\\n\\ntitle: {title}\\n\\nlink: {link}\\n\\ndate: {date}\\n\\n---\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to create the destination directories for the cleaned Markdown files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_directory_structure_only(src, dest):\n",
    "    \"\"\"\n",
    "    Copies only the directory structure from src to dest.\n",
    "    \n",
    "    Parameters:\n",
    "        src (str): source directory to be recursively crawled through and duplicated\n",
    "        dest (str): destination directory where the cleaned Markdown files will go\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ensure the base destination directory exists\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest)\n",
    "\n",
    "    for root, dirs, files in os.walk(src):\n",
    "        # Calculate the relative path from the source directory to the current directory\n",
    "        rel_path = os.path.relpath(root, src)\n",
    "        # Construct the corresponding path in the destination\n",
    "        dest_path = os.path.join(dest, rel_path)\n",
    "        if not os.path.exists(dest_path):\n",
    "            os.makedirs(dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can clean each file. We upload the file, create a thread for processing, add the metadata to the response and destroy the message history and uploaded file to prevent the agent from referencing other documents when processing a particular file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_markdown_file(old_file_path, new_file_path):\n",
    "    \"\"\"\n",
    "    Process a single markdown file by sending its contents to the OpenAI API, followed by writing it to a Markdown file and destroying the message and file history.\n",
    "    \n",
    "    Parameters:\n",
    "        old_file_path (str): File path of the original uncleaned markdown file\n",
    "        new_file_path (str): Destination path of the final cleaned markdown file\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Upload the markdown file to be accessible by the API\n",
    "    if os.path.exists(new_file_path):\n",
    "        return 1\n",
    "    with open(old_file_path, \"rb\") as file:\n",
    "        file = client.files.create(file=file, purpose=\"assistants\")\n",
    "\n",
    "    # Create a thread for processing\n",
    "    thread = client.beta.threads.create()\n",
    "    thread_message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=\"Return the Markdown and links in-line. Take note of any rowspan or colspan in tables. Do not give any welcome or ending message, only the Markdown output. Parse the whole document, including the contents of the tables. Return the Markdown and links in-line. Take note of any rowspan or colspan in tables. Do not give any welcome or ending message, only the Markdown output. Parse the whole document, including the contents of the tables. Do your best. Do your best. \",\n",
    "        attachments=[{\"file_id\": file.id, \"tools\": [{\"type\": \"file_search\"}]}]\n",
    "    )\n",
    "\n",
    "    class EventHandler(AssistantEventHandler):\n",
    "        @override\n",
    "        # Wait for GPT to complete its response before taking all its output\n",
    "        def on_message_done(self, message) -> None:\n",
    "            message_content = message.content[0].text\n",
    "            message_content = add_yaml_metadata(\n",
    "                message_content, new_file_path)\n",
    "            with open(new_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"{message_content.value}\\n\")\n",
    "\n",
    "    # Listen to the thread and process events\n",
    "    with client.beta.threads.runs.stream(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        event_handler=EventHandler(),\n",
    "    ) as stream:\n",
    "        stream.until_done()\n",
    "\n",
    "    # Clean up: delete the message and the uploaded file\n",
    "    client.beta.threads.messages.delete(\n",
    "        message_id=thread_message.id, thread_id=thread.id)\n",
    "    client.files.delete(file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can run the cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dir = '..\\\\data\\\\Development-Control-md'\n",
    "new_dir = '..\\\\data\\\\DC-cleaned-md'\n",
    "\n",
    "# Copy directory structure without files\n",
    "copy_directory_structure_only(original_dir, new_dir)\n",
    "\n",
    "# Process each Markdown file\n",
    "for root, dirs, files in os.walk(original_dir):\n",
    "    for name in files:\n",
    "        if name.endswith('.md'):\n",
    "            old_md_path = os.path.join(root, name)\n",
    "            new_md_path = old_md_path.replace(original_dir, new_dir)\n",
    "            try:\n",
    "                task = process_markdown_file(old_md_path, new_md_path)\n",
    "                if task == 1:\n",
    "                    print(f\"Skipped API call: {new_md_path}\")\n",
    "                else:\n",
    "                    print(f\"Successfully cleaned to {new_md_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to clean to {new_md_path}\")\n",
    "                print(e.message, e.args)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
